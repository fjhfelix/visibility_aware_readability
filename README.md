# Visibility-Aware Readability (Human–Machine Systems)

This repository contains **reproducible, publication-grade code** for studying **text readability feasibility**
under controlled, classroom-like viewing configurations.  
The work is situated in **human–machine systems**, with a deliberate focus on *perceptual accessibility* rather
than intent, cognition, or downstream behavior.

The goal is to model whether text is **readable in principle** (`can_read`) given a viewing setup
(distance, angular size, head pose, medium, contrast), and to evaluate whether a **human-rated visibility score**
is *predictively sufficient* relative to richer feature sets.

The repository is designed and implemented,  emphasizing:
- interpretability,
- deployment-relevant validation,
- uncertainty-aware reporting,
- and statistically principled sufficiency analyses.

##  TL;DR
We study whether text is *readable in principle* under controlled viewing conditions. 
We evaluate whether a human-rated visibility score is predictively sufficient
relative to geometric and environmental features, using leakage-safe, uncertainty-aware validation.

---

## Repository Organization

This GitHub repository is intentionally split into **two branches**:

- **`Code`**  
  Contains *all executable source code* used in the study.

- **`Result`**  
  Contains *all artifacts generated by the code*:  
  CSV tables, figures, and LaTeX files corresponding exactly to reported results.

This separation ensures **clean reproducibility**, transparent auditing, and reviewer-friendly inspection.

---

## Scope and Interpretation (Important)

> **Scope note — read carefully.**

The dataset includes a **human-rated visibility score** that acts as a perceptual proxy for readability.
Because this score is derived from human judgment, it is **conceptually correlated** with the binary label
`can_read`.

As a result:
- All analyses should be interpreted as **predictive sufficiency within this measurement regime**.
- The results **do not claim causal sufficiency** nor guaranteed deployability in unconstrained classrooms.
- The contribution is methodological: *how well visibility explains readability relative to geometry and conditions,
  under careful validation*.

---

## What This Repository Reproduces

Running the code reproduces all components reported in the manuscript.

### 1. Scenario-Level Evaluation
- Repeated stratified cross-validation with pooled out-of-fold (OOF) predictions
- Condition-blocked cross-validation across configuration groups  
  (deployment-shift proxy)

### 2. Interpretable Modeling
- Logistic Regression (L2-regularized)
- Random Forest classifiers
- Consistent feature preprocessing and leakage-safe pipelines

### 3. Probability Calibration
- Uncalibrated baseline
- Platt scaling
- Isotonic regression  
(All fitted on training-only predictions)

### 4. Explainability
- Impurity-based feature importance
- Permutation importance
- Partial Dependence Plots (1D and 2D)

### 5. Sufficiency-Oriented Analyses (Predictive, Not Causal)
- Feature ablation:
  - Geometry-only
  - Visibility-only
  - Geometry + Visibility
- Paired AUC comparisons (pooled OOF predictions)
- Residualization-style stress tests
- Information-theoretic summaries (as implemented)

### 6. Robustness Diagnostics
- Structured perturbations:
  - visibility bias and clipping
  - pose quantization
  - feature noise
- Performance degradation tracking

### 7. Uncertainty-Aware Evaluation
- Bootstrap confidence intervals
- Subgroup and worst-case reporting
- Split conformal prediction under scenario-level splits
- Mondrian (group-conditional) conformal coverage

---

## Dataset Assumptions

All scripts expect a CSV file named: `reading_visibility_dataset_300rows.csv`


### Required Columns

**Target**
- `can_read` (binary)

**Visibility**
- `visibility_score`

**Geometry**
- `distance_m`
- `font_size_pt`
- `text_height_mm`
- `angular_size_deg`

**Head Pose**
- `head_yaw_deg`
- `head_pitch_deg`
- `head_roll_deg`

**Viewing Conditions**
- `medium`
- `contrast`

**Optional**
- `participant_id`  
  If absent, some scripts generate balanced pseudo-participants strictly for
  stress-testing and LOPO-style evaluation. These are *not* treated as real subjects.

---

## Installation

### Install Recommend
`pip install -U numpy pandas scikit-learn scipy matplotlib tqdm seaborn`


### `thms_pipeline.py` 

This is the **primary script** used for manuscript-quality results.

python thms_pipeline.py
## What It Does
- Repeated stratified cross-validation with uncertainty estimation
- Condition-blocked generalization tests
- Logistic Regression and Random Forest baselines
- Isotonic probability calibration
- Sufficiency ablation with paired statistical testing
- Subgroup and worst-case performance analysis
- Robustness stress tests
- Conformal prediction (including Mondrian variants)
- Cross-fitted residual sufficiency tests
- IEEE-ready LaTeX tables
  
---

### Additional Scripts

### `thms_full.py`

A Colab-oriented version of the full pipeline with identical scientific intent.
This script is intended for cloud execution and reproducibility.
  
---

### `perceptual_sufficiency_evaluation.py`

A standalone evaluation laboratory focusing on pooled out-of-fold (OOF) predictions
and statistical sufficiency testing.

## Includes:
- LOPO-style group evaluation (when available)
- Paired DeLong AUC tests
- Bootstrap confidence intervals
- Calibration diagnostics
- Decision curve analysis
- Robustness to feature and label noise
- Conformal classification
- Cross-fitted residual testing
  
---

### `Project.py`

Exploratory analysis and baseline modeling script.

## Includes:

- Exploratory data analysis (EDA) plots
- Baseline classifiers
- ROC curves and confusion matrices
- Calibration curves
- Feature importance analysis
- Partial Dependence Plots (PDPs)
  
---

### `Ablation_AUCperPaticipant.py`

A didactic ablation and LOPO-style demonstration script.

## Includes:

- Simple train/test ablation
- Pseudo-participant assignment
- Per-participant AUC visualization
This script is illustrative and not the primary reporting pipeline.
  
---

### Reproducibility

- Fixed random seeds (`RANDOM_STATE = 42`) throughout
- Leakage-safe preprocessing
- Deterministic outputs except where stochastic repetition is intentional
- Clear separation of code and results via GitHub branches









